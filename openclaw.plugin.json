{
  "id": "nuron",
  "name": "Nuron",
  "description": "Temporal knowledge graph memory for OpenClaw with adaptive importance scoring (Memory Cortex)",
  "kind": "memory",
  "version": "2.0.0",
  "author": "Basuru",
  "configSchema": {
    "type": "object",
    "additionalProperties": false,
    "properties": {
      "backend": {
        "type": "string",
        "enum": ["auto", "graphiti-mcp", "neo4j"],
        "default": "auto",
        "description": "Memory backend to use: auto-detect, graphiti-mcp, or neo4j"
      },
      "endpoint": {
        "type": "string",
        "default": "http://localhost:8000/sse",
        "description": "Graphiti MCP server endpoint (for graphiti-mcp backend). When transport is 'sse', this should include the /sse path."
      },
      "transport": {
        "type": "string",
        "enum": ["stdio", "sse"],
        "default": "sse",
        "description": "Graphiti MCP transport type"
      },
      "groupId": {
        "type": "string",
        "default": "default",
        "description": "Memory group ID for all conversations"
      },
      "neo4j": {
        "type": "object",
        "properties": {
          "uri": { "type": "string", "default": "bolt://localhost:7687", "description": "Neo4j Bolt connection URI" },
          "user": { "type": "string", "default": "neo4j", "description": "Neo4j username" },
          "password": { "type": "string", "description": "Neo4j password" },
          "database": { "type": "string", "description": "Neo4j database name" }
        },
        "description": "Neo4j connection settings (for neo4j backend)"
      },
      "autoCapture": {
        "type": "boolean",
        "default": true,
        "description": "Automatically capture conversations to memory"
      },
      "autoRecall": {
        "type": "boolean",
        "default": true,
        "description": "Automatically recall relevant memories before each response"
      },
      "recallMaxFacts": {
        "type": "number",
        "default": 5,
        "minimum": 1,
        "maximum": 20,
        "description": "Maximum facts to recall per query"
      },
      "minPromptLength": {
        "type": "number",
        "default": 20,
        "minimum": 1,
        "description": "Minimum prompt length to trigger recall"
      },
      "scoringEnabled": {
        "type": "boolean",
        "default": true,
        "description": "Enable adaptive importance scoring"
      },
      "scoringExplicitThreshold": {
        "type": "number",
        "default": 8,
        "minimum": 1,
        "maximum": 10,
        "description": "Score >= this triggers explicit storage. Must be > scoringEphemeralThreshold."
      },
      "scoringEphemeralThreshold": {
        "type": "number",
        "default": 4,
        "minimum": 0,
        "maximum": 9,
        "description": "Score < this triggers ephemeral storage. Must be < scoringExplicitThreshold."
      },
      "scoringEphemeralHours": {
        "type": "number",
        "default": 72,
        "minimum": 1,
        "maximum": 168,
        "description": "Hours before ephemeral memories auto-expire"
      },
      "scoringSilentDays": {
        "type": "number",
        "default": 30,
        "minimum": 1,
        "maximum": 365,
        "description": "Days before silent memories need reinforcement"
      },
      "scoringCleanupHours": {
        "type": "number",
        "default": 12,
        "minimum": 1,
        "maximum": 24,
        "description": "How often to run cleanup via heartbeat (hours)"
      },
      "scoringNotifyExplicit": {
        "type": "boolean",
        "default": true,
        "description": "Notify user when storing explicit importance memories"
      },
      "scoringAskBeforeDowngrade": {
        "type": "boolean",
        "default": true,
        "description": "Ask user before downgrading memory importance"
      },
      "scoringMinConversationLength": {
        "type": "number",
        "default": 50,
        "minimum": 0,
        "description": "Minimum total character length across all messages to trigger scoring. Shorter conversations default to ephemeral."
      },
      "scoringMinMessageCount": {
        "type": "number",
        "default": 1,
        "minimum": 1,
        "description": "Minimum number of messages required to trigger scoring"
      },
      "scoringDefaultTier": {
        "type": "string",
        "enum": ["explicit", "silent", "ephemeral"],
        "default": "silent",
        "description": "Default memory tier when scoringEnabled is false (dumb mode)"
      },
      "scoringModel": {
        "type": "object",
        "properties": {
          "provider": {
            "type": "string",
            "enum": ["llamacpp", "openai", "none"],
            "default": "none",
            "description": "Scoring model provider. llamacpp for a llama.cpp server, openai for any OpenAI-compatible API, none for built-in heuristics"
          },
          "model": {
            "type": "string",
            "description": "Model name sent in the API request"
          },
          "endpoint": {
            "type": "string",
            "default": "http://localhost:8080",
            "description": "Scoring model server URL"
          },
          "apiKey": {
            "type": "string",
            "description": "API key (required for openai provider, optional for llamacpp)"
          },
          "timeoutMs": {
            "type": "number",
            "default": 10000,
            "minimum": 1000,
            "description": "Request timeout in milliseconds"
          }
        },
        "description": "Optional local model for importance scoring instead of heuristics"
      }
    }
  },
  "uiHints": {
    "neo4j.password": { "label": "Neo4j Password", "sensitive": true },
    "scoringModel.apiKey": { "label": "Scoring Model API Key", "sensitive": true }
  }
}
